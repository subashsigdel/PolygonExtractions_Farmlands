{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8f4a0-5563-45c7-8dbd-10a1a02dcdd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import cv2\n",
    "from rasterio.features import shapes\n",
    "from shapely.geometry import shape, mapping\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "mask_path = \"outputs/tile_24576_24576_masks.tif\"\n",
    "clean_mask_path = \"masks_clean.tif\"\n",
    "boundaries_path = \"masks_boundaries.tif\"\n",
    "polygons_geojson = \"polygons.geojson\"\n",
    "\n",
    "# --- Watershed Separation ---\n",
    "USE_WATERSHED_SEPARATION = False\n",
    "WATERSHED_THRESHOLD = 0.25\n",
    "\n",
    "# --- Morphological Processing ---\n",
    "USE_MORPHOLOGY = True\n",
    "MORPH_CLOSE_KERNEL = 3\n",
    "MORPH_CLOSE_ITERATIONS = 1\n",
    "\n",
    "# DILATION STRATEGY: Expand objects to close gaps\n",
    "MORPH_DILATE_KERNEL = 5      # Expand by 5px (closes gaps before merging!)\n",
    "MORPH_DILATE_ITERATIONS = 1\n",
    "\n",
    "# EROSION: Shrink back after merging (MUST MATCH DILATION)\n",
    "MORPH_ERODE_KERNEL = 5       # Shrink by 5px (after merging)\n",
    "MORPH_ERODE_ITERATIONS = 1\n",
    "\n",
    "# --- SMART MERGING (Two-Pass) ---\n",
    "USE_SMART_MERGING = True\n",
    "MERGE_THRESHOLD = 30000\n",
    "MERGE_SEARCH_DISTANCE = 8\n",
    "\n",
    "# --- Filtering Parameters (Applied AFTER merging) ---\n",
    "MIN_AREA = 5000\n",
    "MAX_AREA = 20000000\n",
    "\n",
    "# Shape quality filters\n",
    "MIN_COMPACTNESS = 0.005\n",
    "MIN_SOLIDITY = 0.4\n",
    "MAX_ASPECT_RATIO = 150.0\n",
    "MIN_CONVEXITY = 0.4\n",
    "MIN_EXTENT = 0.1\n",
    "\n",
    "BOUNDARY_THICKNESS = 5\n",
    "CREATE_BOUNDARY_FILE = True\n",
    "SAVE_GEOJSON = True\n",
    "\n",
    "def load_sam_mask(mask_path):\n",
    "    \"\"\"Load SAM mask and convert to instance mask\"\"\"\n",
    "    with rasterio.open(mask_path) as src:\n",
    "        mask_data = src.read()\n",
    "        profile = src.profile.copy()\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "        \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MASK ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Shape: {mask_data.shape}\")\n",
    "    print(f\"Dtype: {mask_data.dtype}\")\n",
    "    print(f\"Bands: {mask_data.shape[0] if mask_data.ndim == 3 else 1}\")\n",
    "    \n",
    "    if mask_data.ndim == 3 and mask_data.shape[0] > 1:\n",
    "        print(f\"\\nâœ“ Multi-band mask: {mask_data.shape[0]} bands\")\n",
    "        instance_mask = np.zeros(mask_data.shape[1:], dtype=np.uint16)\n",
    "        print(f\"\\nAnalyzing bands:\")\n",
    "        valid_bands = 0\n",
    "        for band_idx in range(mask_data.shape[0]):\n",
    "            band = mask_data[band_idx]\n",
    "            nonzero = np.count_nonzero(band > 0)\n",
    "            if nonzero > 0:\n",
    "                valid_bands += 1\n",
    "                band_mask = band > 0\n",
    "                instance_mask[band_mask & (instance_mask == 0)] = band_idx + 1\n",
    "                if band_idx < 10:\n",
    "                    print(f\"  Band {band_idx+1}: {nonzero:,} pixels\")\n",
    "        if mask_data.shape[0] > 10:\n",
    "            print(f\"  ... and {mask_data.shape[0] - 10} more bands\")\n",
    "        print(f\"\\nâœ“ Converted {valid_bands} bands\")\n",
    "    else:\n",
    "        instance_mask = mask_data.squeeze()\n",
    "        unique_vals = np.unique(instance_mask)\n",
    "        unique_vals = unique_vals[unique_vals > 0]\n",
    "        if len(unique_vals) > 1:\n",
    "            print(f\"\\nâœ“ Single-band instance mask: {len(unique_vals)} objects\")\n",
    "        else:\n",
    "            print(f\"\\nâš  Binary mask - separating connected regions\")\n",
    "            instance_mask, num = ndimage.label(instance_mask > 0)\n",
    "            print(f\"â†’ Found {num} connected regions\")\n",
    "    \n",
    "    num_instances = len(np.unique(instance_mask)) - 1\n",
    "    total_pixels = np.sum(instance_mask > 0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULT: {num_instances} objects, {total_pixels:,} pixels\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return instance_mask, profile, transform, crs\n",
    "\n",
    "def apply_watershed_separation(instance_mask):\n",
    "    \"\"\"Apply watershed algorithm to separate touching objects\"\"\"\n",
    "    if not USE_WATERSHED_SEPARATION:\n",
    "        return instance_mask\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"WATERSHED SEPARATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    binary_mask = (instance_mask > 0).astype(np.uint8)\n",
    "    dist = cv2.distanceTransform(binary_mask, cv2.DIST_L2, 3)\n",
    "    dist_norm = cv2.normalize(dist, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    _, peaks = cv2.threshold(dist_norm, WATERSHED_THRESHOLD * dist_norm.max(), 255, cv2.THRESH_BINARY)\n",
    "    peaks = peaks.astype(np.uint8)\n",
    "    _, markers = cv2.connectedComponents(peaks)\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    markers = cv2.dilate(markers.astype(np.uint8), kernel, iterations=1)\n",
    "    markers = markers.astype(np.int32)\n",
    "    markers[binary_mask == 0] = 0\n",
    "    binary_3ch = cv2.cvtColor(binary_mask * 255, cv2.COLOR_GRAY2BGR)\n",
    "    markers = cv2.watershed(binary_3ch, markers)\n",
    "    separated_mask = np.where(markers > 0, markers, 0).astype(np.uint16)\n",
    "    \n",
    "    before_count = len(np.unique(instance_mask)) - 1\n",
    "    after_count = len(np.unique(separated_mask)) - 1\n",
    "    \n",
    "    print(f\"âœ“ Threshold: {WATERSHED_THRESHOLD}\")\n",
    "    print(f\"  Objects before: {before_count}\")\n",
    "    print(f\"  Objects after: {after_count}\")\n",
    "    \n",
    "    return separated_mask\n",
    "\n",
    "def apply_pre_merge_morphology(instance_mask):\n",
    "    \"\"\"Pre-merge: Fill holes and dilate to close gaps\"\"\"\n",
    "    if not USE_MORPHOLOGY:\n",
    "        return instance_mask\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MORPHOLOGICAL PROCESSING (Pre-Merge)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    improved_mask = np.zeros_like(instance_mask, dtype=np.uint16)\n",
    "    \n",
    "    for inst_id in unique_ids:\n",
    "        obj_mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        \n",
    "        # Closing: Fill holes\n",
    "        if MORPH_CLOSE_KERNEL > 0 and MORPH_CLOSE_ITERATIONS > 0:\n",
    "            close_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, \n",
    "                                                     (MORPH_CLOSE_KERNEL, MORPH_CLOSE_KERNEL))\n",
    "            obj_mask = cv2.morphologyEx(obj_mask, cv2.MORPH_CLOSE, close_kernel, \n",
    "                                       iterations=MORPH_CLOSE_ITERATIONS)\n",
    "        \n",
    "        # Dilation: Expand to close gaps\n",
    "        if MORPH_DILATE_KERNEL > 0 and MORPH_DILATE_ITERATIONS > 0:\n",
    "            dilate_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, \n",
    "                                                      (MORPH_DILATE_KERNEL, MORPH_DILATE_KERNEL))\n",
    "            obj_mask = cv2.dilate(obj_mask, dilate_kernel, iterations=MORPH_DILATE_ITERATIONS)\n",
    "        \n",
    "        improved_mask[obj_mask > 0] = inst_id\n",
    "    \n",
    "    print(f\"âœ“ Applied pre-merge operations:\")\n",
    "    if MORPH_CLOSE_KERNEL > 0:\n",
    "        print(f\"  â€¢ Closing: kernel={MORPH_CLOSE_KERNEL}, iterations={MORPH_CLOSE_ITERATIONS}\")\n",
    "    if MORPH_DILATE_KERNEL > 0:\n",
    "        print(f\"  â€¢ Dilation: kernel={MORPH_DILATE_KERNEL}, iterations={MORPH_DILATE_ITERATIONS}\")\n",
    "        print(f\"    â†’ Objects expanded by ~{MORPH_DILATE_KERNEL}px (closes gaps!)\")\n",
    "    \n",
    "    before_pixels = np.sum(instance_mask > 0)\n",
    "    after_pixels = np.sum(improved_mask > 0)\n",
    "    change = after_pixels - before_pixels\n",
    "    change_pct = (change / before_pixels * 100) if before_pixels > 0 else 0\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Pixel changes:\")\n",
    "    print(f\"  Before: {before_pixels:,} pixels\")\n",
    "    print(f\"  After: {after_pixels:,} pixels\")\n",
    "    print(f\"  Change: {change:+,} pixels ({change_pct:+.1f}%)\")\n",
    "    \n",
    "    return improved_mask\n",
    "\n",
    "def apply_post_merge_morphology(instance_mask):\n",
    "    \"\"\"Post-merge: Erode to restore original size\"\"\"\n",
    "    if not USE_MORPHOLOGY:\n",
    "        return instance_mask\n",
    "    \n",
    "    if MORPH_ERODE_KERNEL == 0:\n",
    "        print(f\"\\nâš  Post-merge erosion disabled (MORPH_ERODE_KERNEL = 0)\")\n",
    "        return instance_mask\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MORPHOLOGICAL PROCESSING (Post-Merge)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    improved_mask = np.zeros_like(instance_mask, dtype=np.uint16)\n",
    "    \n",
    "    for inst_id in unique_ids:\n",
    "        obj_mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        \n",
    "        # Erosion: Shrink back to original size\n",
    "        if MORPH_ERODE_KERNEL > 0 and MORPH_ERODE_ITERATIONS > 0:\n",
    "            erode_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, \n",
    "                                                     (MORPH_ERODE_KERNEL, MORPH_ERODE_KERNEL))\n",
    "            obj_mask = cv2.erode(obj_mask, erode_kernel, iterations=MORPH_ERODE_ITERATIONS)\n",
    "        \n",
    "        improved_mask[obj_mask > 0] = inst_id\n",
    "    \n",
    "    print(f\"âœ“ Applied post-merge operations:\")\n",
    "    print(f\"  â€¢ Erosion: kernel={MORPH_ERODE_KERNEL}, iterations={MORPH_ERODE_ITERATIONS}\")\n",
    "    print(f\"    â†’ Objects shrunk back by ~{MORPH_ERODE_KERNEL}px\")\n",
    "    \n",
    "    before_pixels = np.sum(instance_mask > 0)\n",
    "    after_pixels = np.sum(improved_mask > 0)\n",
    "    change = after_pixels - before_pixels\n",
    "    change_pct = (change / before_pixels * 100) if before_pixels > 0 else 0\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Pixel changes:\")\n",
    "    print(f\"  Before: {before_pixels:,} pixels\")\n",
    "    print(f\"  After: {after_pixels:,} pixels\")\n",
    "    print(f\"  Change: {change:+,} pixels ({change_pct:+.1f}%)\")\n",
    "    \n",
    "    return improved_mask\n",
    "\n",
    "def merge_small_objects(instance_mask):\n",
    "    \"\"\"Two-pass merging: smallâ†’small, then smallâ†’large\"\"\"\n",
    "    if not USE_SMART_MERGING:\n",
    "        return instance_mask\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SMART MERGING (Two-Pass Strategy)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Merge threshold: {MERGE_THRESHOLD:,} pixels\")\n",
    "    print(f\"Search distance: {MERGE_SEARCH_DISTANCE}px\")\n",
    "    \n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    # Calculate initial areas\n",
    "    object_areas = {}\n",
    "    for obj_id in unique_ids:\n",
    "        area = np.sum(instance_mask == obj_id)\n",
    "        object_areas[obj_id] = area\n",
    "    \n",
    "    # Classify objects\n",
    "    small_objects = set([obj_id for obj_id, area in object_areas.items() if area < MERGE_THRESHOLD])\n",
    "    large_objects = set([obj_id for obj_id, area in object_areas.items() if area >= MERGE_THRESHOLD])\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Initial classification:\")\n",
    "    print(f\"  Large objects (â‰¥{MERGE_THRESHOLD:,}px): {len(large_objects)}\")\n",
    "    print(f\"  Small objects (<{MERGE_THRESHOLD:,}px): {len(small_objects)}\")\n",
    "    \n",
    "    if len(small_objects) == 0:\n",
    "        print(f\"\\nâœ“ No small objects to merge!\")\n",
    "        return instance_mask\n",
    "    \n",
    "    search_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, \n",
    "                                              (MERGE_SEARCH_DISTANCE, MERGE_SEARCH_DISTANCE))\n",
    "    \n",
    "    # PASS 1: Small â†’ Small\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(f\"PASS 1: Merging Small â†’ Small\")\n",
    "    print(f\"{'â”€'*60}\")\n",
    "    \n",
    "    merged_mask = instance_mask.copy()\n",
    "    merge_map = {}\n",
    "    small_to_small_merges = 0\n",
    "    \n",
    "    sorted_small = sorted(small_objects, key=lambda x: object_areas[x], reverse=True)\n",
    "    \n",
    "    for small_id in sorted_small:\n",
    "        if small_id in merge_map:\n",
    "            continue\n",
    "        \n",
    "        small_mask = (merged_mask == small_id).astype(np.uint8)\n",
    "        dilated = cv2.dilate(small_mask, search_kernel, iterations=1)\n",
    "        neighbor_region = dilated > 0\n",
    "        neighbor_ids = np.unique(merged_mask[neighbor_region])\n",
    "        neighbor_ids = neighbor_ids[(neighbor_ids > 0) & (neighbor_ids != small_id)]\n",
    "        small_neighbors = [nid for nid in neighbor_ids \n",
    "                          if nid in small_objects and nid not in merge_map]\n",
    "        \n",
    "        if len(small_neighbors) > 0:\n",
    "            for neighbor_id in small_neighbors:\n",
    "                neighbor_mask = merged_mask == neighbor_id\n",
    "                merged_mask[neighbor_mask] = small_id\n",
    "                merge_map[neighbor_id] = small_id\n",
    "                small_to_small_merges += 1\n",
    "    \n",
    "    print(f\"âœ“ Pass 1 complete: {small_to_small_merges} merges\")\n",
    "    \n",
    "    # Recalculate areas\n",
    "    object_areas_pass1 = {}\n",
    "    remaining_ids = np.unique(merged_mask)\n",
    "    remaining_ids = remaining_ids[remaining_ids > 0]\n",
    "    \n",
    "    for obj_id in remaining_ids:\n",
    "        area = np.sum(merged_mask == obj_id)\n",
    "        object_areas_pass1[obj_id] = area\n",
    "    \n",
    "    small_objects_pass1 = set([obj_id for obj_id, area in object_areas_pass1.items() \n",
    "                               if area < MERGE_THRESHOLD])\n",
    "    large_objects_pass1 = set([obj_id for obj_id, area in object_areas_pass1.items() \n",
    "                               if area >= MERGE_THRESHOLD])\n",
    "    \n",
    "    promoted = len(large_objects_pass1) - len(large_objects)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š After Pass 1:\")\n",
    "    print(f\"  Large objects: {len(large_objects_pass1)} (+{promoted} promoted)\")\n",
    "    print(f\"  Small objects: {len(small_objects_pass1)} (remaining)\")\n",
    "    \n",
    "    # PASS 2: Small â†’ Large\n",
    "    print(f\"\\n{'â”€'*60}\")\n",
    "    print(f\"PASS 2: Merging Small â†’ Large\")\n",
    "    print(f\"{'â”€'*60}\")\n",
    "    \n",
    "    small_to_large_merges = 0\n",
    "    orphan_count = 0\n",
    "    \n",
    "    for small_id in small_objects_pass1:\n",
    "        small_mask = (merged_mask == small_id).astype(np.uint8)\n",
    "        dilated = cv2.dilate(small_mask, search_kernel, iterations=1)\n",
    "        neighbor_region = dilated > 0\n",
    "        neighbor_ids = np.unique(merged_mask[neighbor_region])\n",
    "        neighbor_ids = neighbor_ids[(neighbor_ids > 0) & (neighbor_ids != small_id)]\n",
    "        large_neighbors = [nid for nid in neighbor_ids if nid in large_objects_pass1]\n",
    "        \n",
    "        if len(large_neighbors) > 0:\n",
    "            neighbor_areas = [(nid, object_areas_pass1[nid]) for nid in large_neighbors]\n",
    "            target_id = max(neighbor_areas, key=lambda x: x[1])[0]\n",
    "            merged_mask[small_mask > 0] = target_id\n",
    "            small_to_large_merges += 1\n",
    "        else:\n",
    "            orphan_count += 1\n",
    "    \n",
    "    print(f\"âœ“ Pass 2 complete: {small_to_large_merges} merges, {orphan_count} orphans\")\n",
    "    \n",
    "    before_count = len(unique_ids)\n",
    "    after_count = len(np.unique(merged_mask)) - 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MERGING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Objects before: {before_count}\")\n",
    "    print(f\"  Objects after: {after_count}\")\n",
    "    print(f\"  Eliminated: {before_count - after_count}\")\n",
    "    print(f\"    â€¢ Smallâ†’Small: {small_to_small_merges}\")\n",
    "    print(f\"    â€¢ Smallâ†’Large: {small_to_large_merges}\")\n",
    "    print(f\"    â€¢ Orphans: {orphan_count}\")\n",
    "    \n",
    "    return merged_mask\n",
    "\n",
    "def calculate_shape_metrics(obj_mask):\n",
    "    \"\"\"Calculate shape quality metrics\"\"\"\n",
    "    contours, _ = cv2.findContours(obj_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    \n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "    area = cv2.contourArea(contour)\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    \n",
    "    if area == 0 or perimeter == 0:\n",
    "        return None\n",
    "    \n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    hull = cv2.convexHull(contour)\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    hull_perimeter = cv2.arcLength(hull, True)\n",
    "    \n",
    "    metrics = {\n",
    "        'area': area,\n",
    "        'perimeter': perimeter,\n",
    "        'width': w,\n",
    "        'height': h,\n",
    "        'compactness': (4 * np.pi * area) / (perimeter ** 2) if perimeter > 0 else 0,\n",
    "        'solidity': area / hull_area if hull_area > 0 else 0,\n",
    "        'aspect_ratio': max(w, h) / min(w, h) if min(w, h) > 0 else 0,\n",
    "        'extent': area / (w * h) if (w * h) > 0 else 0,\n",
    "        'convexity': hull_perimeter / perimeter if perimeter > 0 else 0,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def filter_objects_by_quality(instance_mask):\n",
    "    \"\"\"Filter objects based on size and shape quality\"\"\"\n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    cleaned_mask = np.zeros_like(instance_mask, dtype=np.uint16)\n",
    "    new_id = 1\n",
    "    \n",
    "    removed_reasons = {\n",
    "        'too_small': 0, 'too_large': 0, 'low_compactness': 0,\n",
    "        'low_solidity': 0, 'high_aspect_ratio': 0,\n",
    "        'low_convexity': 0, 'low_extent': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FILTERING OBJECTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Criteria:\")\n",
    "    print(f\"  Area: {MIN_AREA:,} - {MAX_AREA:,} pixels\")\n",
    "    print(f\"  Compactness: â‰¥ {MIN_COMPACTNESS:.3f}\")\n",
    "    print(f\"  Solidity: â‰¥ {MIN_SOLIDITY:.2f}\")\n",
    "    print(f\"  Aspect ratio: â‰¤ {MAX_ASPECT_RATIO:.1f}\")\n",
    "    print(f\"  Convexity: â‰¥ {MIN_CONVEXITY:.2f}\")\n",
    "    print(f\"  Extent: â‰¥ {MIN_EXTENT:.2f}\")\n",
    "    \n",
    "    kept_objects = []\n",
    "    \n",
    "    for inst_id in unique_ids:\n",
    "        obj_mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        metrics = calculate_shape_metrics(obj_mask)\n",
    "        \n",
    "        if metrics is None:\n",
    "            removed_reasons['too_small'] += 1\n",
    "            continue\n",
    "        \n",
    "        keep = True\n",
    "        reason = None\n",
    "        \n",
    "        if metrics['area'] < MIN_AREA:\n",
    "            keep, reason = False, 'too_small'\n",
    "        elif metrics['area'] > MAX_AREA:\n",
    "            keep, reason = False, 'too_large'\n",
    "        elif metrics['compactness'] < MIN_COMPACTNESS:\n",
    "            keep, reason = False, 'low_compactness'\n",
    "        elif metrics['solidity'] < MIN_SOLIDITY:\n",
    "            keep, reason = False, 'low_solidity'\n",
    "        elif metrics['aspect_ratio'] > MAX_ASPECT_RATIO:\n",
    "            keep, reason = False, 'high_aspect_ratio'\n",
    "        elif metrics['convexity'] < MIN_CONVEXITY:\n",
    "            keep, reason = False, 'low_convexity'\n",
    "        elif metrics['extent'] < MIN_EXTENT:\n",
    "            keep, reason = False, 'low_extent'\n",
    "        \n",
    "        if keep:\n",
    "            cleaned_mask[obj_mask > 0] = new_id\n",
    "            kept_objects.append(metrics)\n",
    "            new_id += 1\n",
    "        else:\n",
    "            if reason:\n",
    "                removed_reasons[reason] += 1\n",
    "    \n",
    "    total_removed = sum(removed_reasons.values())\n",
    "    total_kept = new_id - 1\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Results:\")\n",
    "    print(f\"  âœ“ Kept: {total_kept} objects\")\n",
    "    print(f\"  âœ— Removed: {total_removed} objects\")\n",
    "    \n",
    "    if total_removed > 0:\n",
    "        print(f\"\\nðŸ“‹ Removal reasons:\")\n",
    "        for reason, count in removed_reasons.items():\n",
    "            if count > 0:\n",
    "                print(f\"  â€¢ {reason.replace('_', ' ').title()}: {count}\")\n",
    "    \n",
    "    if kept_objects:\n",
    "        areas = [m['area'] for m in kept_objects]\n",
    "        print(f\"\\nðŸ“ˆ Kept objects stats:\")\n",
    "        print(f\"  Area: {min(areas):,.0f} - {max(areas):,.0f} px\")\n",
    "    \n",
    "    return cleaned_mask\n",
    "\n",
    "def extract_polygons_per_object(instance_mask, transform):\n",
    "    \"\"\"Extract ONE polygon per object\"\"\"\n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    polygons = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXTRACTING POLYGONS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for inst_id in unique_ids:\n",
    "        obj_mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        area = np.sum(obj_mask)\n",
    "        \n",
    "        for geom, val in shapes(obj_mask, mask=obj_mask, transform=transform):\n",
    "            if val > 0:\n",
    "                poly = shape(geom)\n",
    "                polygons.append({\n",
    "                    'polygon': poly,\n",
    "                    'id': int(inst_id),\n",
    "                    'area': int(area)\n",
    "                })\n",
    "                break\n",
    "    \n",
    "    print(f\"âœ“ Extracted {len(polygons)} polygons\")\n",
    "    \n",
    "    if polygons:\n",
    "        areas = [p['area'] for p in polygons]\n",
    "        print(f\"  Area range: {min(areas):,} - {max(areas):,} pixels\")\n",
    "    \n",
    "    return polygons\n",
    "\n",
    "def save_geojson(polygons, output_path, crs):\n",
    "    \"\"\"Save polygons to GeoJSON\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for poly_data in polygons:\n",
    "        feature = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\n",
    "                \"id\": poly_data['id'],\n",
    "                \"area_pixels\": poly_data['area']\n",
    "            },\n",
    "            \"geometry\": mapping(poly_data['polygon'])\n",
    "        }\n",
    "        features.append(feature)\n",
    "    \n",
    "    geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"crs\": {\n",
    "            \"type\": \"name\",\n",
    "            \"properties\": {\"name\": str(crs) if crs else \"EPSG:4326\"}\n",
    "        },\n",
    "        \"features\": features\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(geojson, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ“ GeoJSON saved: {output_path}\")\n",
    "    print(f\"  â†’ {len(features)} polygons\")\n",
    "\n",
    "def extract_boundaries(instance_mask, thickness=4):\n",
    "    \"\"\"Extract boundary for each object\"\"\"\n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    boundaries = np.zeros_like(instance_mask, dtype=np.uint8)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    \n",
    "    for inst_id in unique_ids:\n",
    "        obj_mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        edges = cv2.morphologyEx(obj_mask, cv2.MORPH_GRADIENT, kernel)\n",
    "        boundaries = np.maximum(boundaries, edges)\n",
    "    \n",
    "    if thickness > 1:\n",
    "        thick_kernel = np.ones((thickness, thickness), np.uint8)\n",
    "        boundaries = cv2.dilate(boundaries, thick_kernel, iterations=1)\n",
    "    \n",
    "    return boundaries\n",
    "\n",
    "def visualize_results(instance_mask, boundaries, polygons, save_path):\n",
    "    \"\"\"Create visualization\"\"\"\n",
    "    binary_mask = (instance_mask > 0).astype(np.uint8)\n",
    "    num_objects = len(np.unique(instance_mask)) - 1\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(18, 18))\n",
    "    \n",
    "    axs[0, 0].imshow(instance_mask, cmap=\"nipy_spectral\", interpolation='nearest')\n",
    "    axs[0, 0].set_title(f\"Instance Mask (Smart Merge + Dilation)\\n({num_objects} objects)\", \n",
    "                        fontsize=14, fontweight='bold')\n",
    "    axs[0, 0].axis(\"off\")\n",
    "    \n",
    "    rgb = np.stack([binary_mask, binary_mask, binary_mask], axis=-1).astype(float)\n",
    "    rgb[boundaries > 0] = [1.0, 0, 0]\n",
    "    \n",
    "    axs[0, 1].imshow(rgb, interpolation='nearest')\n",
    "    axs[0, 1].set_title(f\"Objects with Boundaries\\n({len(polygons)} polygons)\", \n",
    "                        fontsize=14, fontweight='bold')\n",
    "    axs[0, 1].axis(\"off\")\n",
    "    \n",
    "    axs[1, 0].imshow(binary_mask, cmap=\"gray\", interpolation='nearest')\n",
    "    axs[1, 0].set_title(f\"Binary Mask\", fontsize=14, fontweight='bold')\n",
    "    axs[1, 0].axis(\"off\")\n",
    "    \n",
    "    boundary_display = np.zeros_like(instance_mask, dtype=np.uint8)\n",
    "    boundary_display[boundaries > 0] = 255\n",
    "    \n",
    "    axs[1, 1].imshow(boundary_display, cmap=\"gray\", interpolation='nearest')\n",
    "    axs[1, 1].set_title(f\"Boundaries ({BOUNDARY_THICKNESS}px)\", \n",
    "                        fontsize=14, fontweight='bold')\n",
    "    axs[1, 1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Visualization saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline\"\"\"\n",
    "    \n",
    "    if not Path(mask_path).exists():\n",
    "        raise FileNotFoundError(f\"Mask file not found: {mask_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAM MASK â†’ CLEAN POLYGONS\")\n",
    "    print(f\"WITH DILATION + TWO-PASS MERGING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nInput: {mask_path}\")\n",
    "    \n",
    "    # Load mask\n",
    "    instance_mask, profile, transform, crs = load_sam_mask(mask_path)\n",
    "    \n",
    "    # Watershed (if enabled)\n",
    "    instance_mask = apply_watershed_separation(instance_mask)\n",
    "    \n",
    "    # PRE-MERGE: Fill holes and dilate to close gaps\n",
    "    instance_mask = apply_pre_merge_morphology(instance_mask)\n",
    "    \n",
    "    # MERGE: Two-pass small object merging\n",
    "    instance_mask = merge_small_objects(instance_mask)\n",
    "    \n",
    "    # POST-MERGE: Erode back to original size\n",
    "    instance_mask = apply_post_merge_morphology(instance_mask)\n",
    "    \n",
    "    # FILTER: Remove orphans and bad shapes\n",
    "    instance_mask = filter_objects_by_quality(instance_mask)\n",
    "    \n",
    "    # Extract polygons\n",
    "    polygons = extract_polygons_per_object(instance_mask, transform)\n",
    "    \n",
    "    # Extract boundaries\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXTRACTING BOUNDARIES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    boundaries = extract_boundaries(instance_mask, BOUNDARY_THICKNESS)\n",
    "    print(f\"âœ“ Boundary pixels: {np.sum(boundaries > 0):,}\")\n",
    "    \n",
    "    # Save files\n",
    "    profile.update({'count': 1, 'dtype': 'uint16', 'compress': 'lzw', 'nodata': 0})\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAVING FILES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    with rasterio.open(clean_mask_path, \"w\", **profile) as dst:\n",
    "        dst.write(instance_mask.astype(np.uint16), 1)\n",
    "    print(f\"âœ“ Instance mask: {clean_mask_path}\")\n",
    "    \n",
    "    if CREATE_BOUNDARY_FILE:\n",
    "        profile_boundary = profile.copy()\n",
    "        profile_boundary['dtype'] = 'uint8'\n",
    "        with rasterio.open(boundaries_path, \"w\", **profile_boundary) as dst:\n",
    "            dst.write(boundaries, 1)\n",
    "        print(f\"âœ“ Boundaries: {boundaries_path}\")\n",
    "    \n",
    "    if SAVE_GEOJSON:\n",
    "        save_geojson(polygons, polygons_geojson, crs)\n",
    "    \n",
    "    # Visualize\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GENERATING VISUALIZATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    visualize_results(instance_mask, boundaries, polygons, \"polygons_visualization.png\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ“ COMPLETE!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nOutput files:\")\n",
    "    print(f\"  1. {clean_mask_path} - Filtered instance mask\")\n",
    "    print(f\"  2. {boundaries_path} - Boundaries\")\n",
    "    print(f\"  3. {polygons_geojson} - Polygons (GeoJSON)\")\n",
    "    print(f\"  4. polygons_visualization.png - Visualization\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Processing:\")\n",
    "    print(f\"  â€¢ Pre-merge dilation ({MORPH_DILATE_KERNEL}px) - closed gaps\")\n",
    "    print(f\"  â€¢ Two-pass merging - consolidated fragments\")\n",
    "    print(f\"  â€¢ Post-merge erosion ({MORPH_ERODE_KERNEL}px) - restored size\")\n",
    "    print(f\"  â€¢ Quality filtering - removed artifacts\")\n",
    "    \n",
    "    return instance_mask, boundaries, polygons\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        instance_mask, boundaries, polygons = main()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "171a75b9-5cbc-4f23-92f7-10920fa2827d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import cv2\n",
    "from rasterio.features import shapes\n",
    "from shapely.geometry import shape, mapping\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "# mask_path = \"outputs/tile_22528_36864_masks.tif\"\n",
    "clean_mask_path = \"masks_clean.tif\"\n",
    "boundaries_path = \"masks_boundaries.tif\"\n",
    "polygons_geojson = \"polygons.geojson\"\n",
    "\n",
    "# --- Watershed Separation (for touching objects) ---\n",
    "USE_WATERSHED_SEPARATION = True  # Separate touching objects using watershed\n",
    "WATERSHED_THRESHOLD = 0.1        # Lower = more aggressive separation (0.1-0.5)\n",
    "\n",
    "# --- Morphological Processing ---\n",
    "USE_MORPHOLOGY = False        # Apply morphological operations to improve shapes\n",
    "MORPH_CLOSE_KERNEL = 3       # Kernel size for closing (fill small holes)\n",
    "MORPH_CLOSE_ITERATIONS = 1   # How many times to apply closing\n",
    "MORPH_DILATE_KERNEL = 0      # Kernel size for dilation (expand objects)\n",
    "MORPH_DILATE_ITERATIONS = 0  # How many times to dilate\n",
    "MORPH_ERODE_KERNEL = 0       # Kernel size for erosion (shrink back slightly)\n",
    "MORPH_ERODE_ITERATIONS = 0   # How many times to erode (use to smooth after dilation)\n",
    "\n",
    "# --- Filtering Parameters ---\n",
    "MIN_AREA = 10000              # Minimum area in pixels (increase to remove smaller blobs)\n",
    "MAX_AREA = 20000000          # Maximum area (remove if too large)\n",
    "\n",
    "# Shape quality filters\n",
    "MIN_COMPACTNESS = 0.01       # Remove irregular shapes (0-1, higher = more compact)\n",
    "MIN_SOLIDITY = 0.05          # Remove fragmented shapes (0-1, area/convex_hull)\n",
    "MAX_ASPECT_RATIO = 100.0      # Remove elongated shapes (width/height ratio)\n",
    "MIN_CONVEXITY = 0.5         # Remove concave/irregular shapes (0-1)\n",
    "MIN_EXTENT = 0.10            # Remove thin/sparse shapes (area/bounding_box)\n",
    "\n",
    "BOUNDARY_THICKNESS = 5      \n",
    "CREATE_BOUNDARY_FILE = True\n",
    "SAVE_GEOJSON = True\n",
    "\n",
    "def load_sam_mask(mask_path):\n",
    "    \"\"\"Load SAM mask and convert to instance mask\"\"\"\n",
    "    with rasterio.open(mask_path) as src:\n",
    "        mask_data = src.read()\n",
    "        profile = src.profile.copy()\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "        \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MASK ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Shape: {mask_data.shape}\")\n",
    "    print(f\"Dtype: {mask_data.dtype}\")\n",
    "    print(f\"Bands: {mask_data.shape[0] if mask_data.ndim == 3 else 1}\")\n",
    "    \n",
    "    # Handle multi-band masks\n",
    "    if mask_data.ndim == 3 and mask_data.shape[0] > 1:\n",
    "        print(f\"\\nâœ“ Multi-band mask: {mask_data.shape[0]} bands\")\n",
    "        \n",
    "        instance_mask = np.zeros(mask_data.shape[1:], dtype=np.uint16)\n",
    "        \n",
    "        print(f\"\\nAnalyzing bands:\")\n",
    "        valid_bands = 0\n",
    "        for band_idx in range(mask_data.shape[0]):\n",
    "            band = mask_data[band_idx]\n",
    "            nonzero = np.count_nonzero(band > 0)\n",
    "            \n",
    "            if nonzero > 0:\n",
    "                valid_bands += 1\n",
    "                band_mask = band > 0\n",
    "                instance_mask[band_mask & (instance_mask == 0)] = band_idx + 1\n",
    "                \n",
    "                if band_idx < 10:\n",
    "                    print(f\"  Band {band_idx+1}: {nonzero:,} pixels\")\n",
    "        \n",
    "        if mask_data.shape[0] > 10:\n",
    "            print(f\"  ... and {mask_data.shape[0] - 10} more bands\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Converted {valid_bands} bands\")\n",
    "        \n",
    "    else:\n",
    "        instance_mask = mask_data.squeeze()\n",
    "        unique_vals = np.unique(instance_mask)\n",
    "        unique_vals = unique_vals[unique_vals > 0]\n",
    "        \n",
    "        if len(unique_vals) > 1:\n",
    "            print(f\"\\nâœ“ Single-band instance mask: {len(unique_vals)} objects\")\n",
    "        else:\n",
    "            print(f\"\\nâš  Binary mask - separating connected regions\")\n",
    "            instance_mask, num = ndimage.label(instance_mask > 0)\n",
    "            print(f\"â†’ Found {num} connected regions\")\n",
    "    \n",
    "    num_instances = len(np.unique(instance_mask)) - 1\n",
    "    total_pixels = np.sum(instance_mask > 0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULT: {num_instances} objects, {total_pixels:,} pixels\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return instance_mask, profile, transform, crs\n",
    "\n",
    "def apply_watershed_separation(instance_mask):\n",
    "    \"\"\"\n",
    "    Apply watershed algorithm to separate touching objects\n",
    "    This is useful when SAM detects objects that are touching/overlapping\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    instance_mask : numpy.ndarray\n",
    "        Instance mask where objects might be touching\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray : Instance mask with separated objects\n",
    "    \"\"\"\n",
    "    if not USE_WATERSHED_SEPARATION:\n",
    "        return instance_mask\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"WATERSHED SEPARATION (Separate Touching Objects)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create binary mask of all objects\n",
    "    binary_mask = (instance_mask > 0).astype(np.uint8)\n",
    "    \n",
    "    # Distance transform to find object centers\n",
    "    dist = cv2.distanceTransform(binary_mask, cv2.DIST_L2, 3)\n",
    "    \n",
    "    # Normalize distance\n",
    "    dist_norm = cv2.normalize(dist, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    \n",
    "    # Find local maxima (object centers) with configurable threshold\n",
    "    _, peaks = cv2.threshold(dist_norm, WATERSHED_THRESHOLD * dist_norm.max(), 255, cv2.THRESH_BINARY)\n",
    "    peaks = peaks.astype(np.uint8)\n",
    "    \n",
    "    # Label the peaks (these become watershed markers)\n",
    "    _, markers = cv2.connectedComponents(peaks)\n",
    "    \n",
    "    # Small dilation of markers to ensure they're inside objects\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    markers = cv2.dilate(markers.astype(np.uint8), kernel, iterations=1)\n",
    "    markers = markers.astype(np.int32)\n",
    "    \n",
    "    # Mark background\n",
    "    markers[binary_mask == 0] = 0\n",
    "    \n",
    "    # Apply watershed\n",
    "    binary_3ch = cv2.cvtColor(binary_mask * 255, cv2.COLOR_GRAY2BGR)\n",
    "    markers = cv2.watershed(binary_3ch, markers)\n",
    "    \n",
    "    # Create separated instance mask\n",
    "    # Watershed boundaries are marked as -1, we set them to 0\n",
    "    separated_mask = np.where(markers > 0, markers, 0).astype(np.uint16)\n",
    "    \n",
    "    # Count objects before and after\n",
    "    before_count = len(np.unique(instance_mask)) - 1\n",
    "    after_count = len(np.unique(separated_mask)) - 1\n",
    "    \n",
    "    print(f\"âœ“ Watershed separation applied:\")\n",
    "    print(f\"  Threshold: {WATERSHED_THRESHOLD}\")\n",
    "    print(f\"  Objects before: {before_count}\")\n",
    "    print(f\"  Objects after: {after_count}\")\n",
    "    print(f\"  New objects separated: {after_count - before_count}\")\n",
    "    \n",
    "    return separated_mask\n",
    "\n",
    "def apply_morphological_operations(instance_mask):\n",
    "    \"\"\"\n",
    "    Apply morphological operations to improve object shapes\n",
    "    - Closing: fills small holes and gaps\n",
    "    - Dilation: expands objects, smooths edges\n",
    "    - Erosion: shrinks objects back (optional, for smoothing)\n",
    "    \"\"\"\n",
    "    if not USE_MORPHOLOGY:\n",
    "        return instance_mask\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MORPHOLOGICAL PROCESSING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    improved_mask = np.zeros_like(instance_mask, dtype=np.uint16)\n",
    "    \n",
    "    for inst_id in unique_ids:\n",
    "        obj_mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        \n",
    "        # 1. Morphological Closing (fill small holes and gaps)\n",
    "        if MORPH_CLOSE_KERNEL > 0 and MORPH_CLOSE_ITERATIONS > 0:\n",
    "            close_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, \n",
    "                                                     (MORPH_CLOSE_KERNEL, MORPH_CLOSE_KERNEL))\n",
    "            obj_mask = cv2.morphologyEx(obj_mask, cv2.MORPH_CLOSE, close_kernel, \n",
    "                                       iterations=MORPH_CLOSE_ITERATIONS)\n",
    "        \n",
    "        # 2. Dilation (expand object, smooth edges)\n",
    "        if MORPH_DILATE_KERNEL > 0 and MORPH_DILATE_ITERATIONS > 0:\n",
    "            dilate_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, \n",
    "                                                      (MORPH_DILATE_KERNEL, MORPH_DILATE_KERNEL))\n",
    "            obj_mask = cv2.dilate(obj_mask, dilate_kernel, iterations=MORPH_DILATE_ITERATIONS)\n",
    "        \n",
    "        # 3. Erosion (shrink back slightly to smooth, optional)\n",
    "        if MORPH_ERODE_KERNEL > 0 and MORPH_ERODE_ITERATIONS > 0:\n",
    "            erode_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, \n",
    "                                                     (MORPH_ERODE_KERNEL, MORPH_ERODE_KERNEL))\n",
    "            obj_mask = cv2.erode(obj_mask, erode_kernel, iterations=MORPH_ERODE_ITERATIONS)\n",
    "        \n",
    "        # Assign to output mask\n",
    "        improved_mask[obj_mask > 0] = inst_id\n",
    "    \n",
    "    print(f\"âœ“ Applied morphological operations:\")\n",
    "    if MORPH_CLOSE_KERNEL > 0:\n",
    "        print(f\"  â€¢ Closing: kernel={MORPH_CLOSE_KERNEL}, iterations={MORPH_CLOSE_ITERATIONS}\")\n",
    "    if MORPH_DILATE_KERNEL > 0:\n",
    "        print(f\"  â€¢ Dilation: kernel={MORPH_DILATE_KERNEL}, iterations={MORPH_DILATE_ITERATIONS}\")\n",
    "    if MORPH_ERODE_KERNEL > 0:\n",
    "        print(f\"  â€¢ Erosion: kernel={MORPH_ERODE_KERNEL}, iterations={MORPH_ERODE_ITERATIONS}\")\n",
    "    \n",
    "    # Show before/after stats\n",
    "    before_pixels = np.sum(instance_mask > 0)\n",
    "    after_pixels = np.sum(improved_mask > 0)\n",
    "    change = after_pixels - before_pixels\n",
    "    change_pct = (change / before_pixels * 100) if before_pixels > 0 else 0\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Pixel changes:\")\n",
    "    print(f\"  Before: {before_pixels:,} pixels\")\n",
    "    print(f\"  After: {after_pixels:,} pixels\")\n",
    "    print(f\"  Change: {change:+,} pixels ({change_pct:+.1f}%)\")\n",
    "    \n",
    "    return improved_mask\n",
    "\n",
    "def calculate_shape_metrics(obj_mask):\n",
    "    \"\"\"\n",
    "    Calculate shape quality metrics for filtering\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Shape metrics (area, compactness, solidity, aspect_ratio, etc.)\n",
    "    \"\"\"\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(obj_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    \n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Basic metrics\n",
    "    area = cv2.contourArea(contour)\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    \n",
    "    if area == 0 or perimeter == 0:\n",
    "        return None\n",
    "    \n",
    "    # Bounding rectangle\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    \n",
    "    # Convex hull\n",
    "    hull = cv2.convexHull(contour)\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    hull_perimeter = cv2.arcLength(hull, True)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'area': area,\n",
    "        'perimeter': perimeter,\n",
    "        'width': w,\n",
    "        'height': h,\n",
    "    }\n",
    "    \n",
    "    # Compactness (circularity): 4Ï€*area/perimeterÂ²\n",
    "    # Perfect circle = 1.0, irregular = closer to 0\n",
    "    metrics['compactness'] = (4 * np.pi * area) / (perimeter ** 2) if perimeter > 0 else 0\n",
    "    \n",
    "    # Solidity: area / convex_hull_area\n",
    "    # Measures how \"solid\" the shape is (no concavities)\n",
    "    metrics['solidity'] = area / hull_area if hull_area > 0 else 0\n",
    "    \n",
    "    # Aspect ratio: width / height\n",
    "    metrics['aspect_ratio'] = max(w, h) / min(w, h) if min(w, h) > 0 else 0\n",
    "    \n",
    "    # Extent: area / bounding_box_area\n",
    "    # Measures how much of the bounding box is filled\n",
    "    bbox_area = w * h\n",
    "    metrics['extent'] = area / bbox_area if bbox_area > 0 else 0\n",
    "    \n",
    "    # Convexity: convex_hull_perimeter / perimeter\n",
    "    metrics['convexity'] = hull_perimeter / perimeter if perimeter > 0 else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def filter_objects_by_quality(instance_mask):\n",
    "    \"\"\"\n",
    "    Filter objects based on size and shape quality\n",
    "    \"\"\"\n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    cleaned_mask = np.zeros_like(instance_mask, dtype=np.uint16)\n",
    "    new_id = 1\n",
    "    \n",
    "    # Statistics\n",
    "    removed_reasons = {\n",
    "        'too_small': 0,\n",
    "        'too_large': 0,\n",
    "        'low_compactness': 0,\n",
    "        'low_solidity': 0,\n",
    "        'high_aspect_ratio': 0,\n",
    "        'low_convexity': 0,\n",
    "        'low_extent': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FILTERING OBJECTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Criteria:\")\n",
    "    print(f\"  Area: {MIN_AREA:,} - {MAX_AREA:,} pixels\")\n",
    "    print(f\"  Compactness: â‰¥ {MIN_COMPACTNESS:.2f}\")\n",
    "    print(f\"  Solidity: â‰¥ {MIN_SOLIDITY:.2f}\")\n",
    "    print(f\"  Aspect ratio: â‰¤ {MAX_ASPECT_RATIO:.1f}\")\n",
    "    print(f\"  Convexity: â‰¥ {MIN_CONVEXITY:.2f}\")\n",
    "    print(f\"  Extent: â‰¥ {MIN_EXTENT:.2f}\")\n",
    "    \n",
    "    kept_objects = []\n",
    "    \n",
    "    for inst_id in unique_ids:\n",
    "        obj_mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        \n",
    "        # Calculate shape metrics\n",
    "        metrics = calculate_shape_metrics(obj_mask)\n",
    "        \n",
    "        if metrics is None:\n",
    "            removed_reasons['too_small'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Apply filters\n",
    "        keep = True\n",
    "        reason = None\n",
    "        \n",
    "        # Area filter\n",
    "        if metrics['area'] < MIN_AREA:\n",
    "            keep = False\n",
    "            reason = 'too_small'\n",
    "        elif metrics['area'] > MAX_AREA:\n",
    "            keep = False\n",
    "            reason = 'too_large'\n",
    "        # Compactness filter (remove irregular blobs)\n",
    "        elif metrics['compactness'] < MIN_COMPACTNESS:\n",
    "            keep = False\n",
    "            reason = 'low_compactness'\n",
    "        # Solidity filter (remove fragmented shapes)\n",
    "        elif metrics['solidity'] < MIN_SOLIDITY:\n",
    "            keep = False\n",
    "            reason = 'low_solidity'\n",
    "        # Aspect ratio filter (remove elongated shapes)\n",
    "        elif metrics['aspect_ratio'] > MAX_ASPECT_RATIO:\n",
    "            keep = False\n",
    "            reason = 'high_aspect_ratio'\n",
    "        # Convexity filter (remove very concave shapes)\n",
    "        elif metrics['convexity'] < MIN_CONVEXITY:\n",
    "            keep = False\n",
    "            reason = 'low_convexity'\n",
    "        # Extent filter (remove sparse/thin shapes)\n",
    "        elif metrics['extent'] < MIN_EXTENT:\n",
    "            keep = False\n",
    "            reason = 'low_extent'\n",
    "        \n",
    "        if keep:\n",
    "            cleaned_mask[obj_mask > 0] = new_id\n",
    "            kept_objects.append(metrics)\n",
    "            new_id += 1\n",
    "        else:\n",
    "            if reason:\n",
    "                removed_reasons[reason] += 1\n",
    "    \n",
    "    total_removed = sum(removed_reasons.values())\n",
    "    total_kept = new_id - 1\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Results:\")\n",
    "    print(f\"  âœ“ Kept: {total_kept} objects\")\n",
    "    print(f\"  âœ— Removed: {total_removed} objects\")\n",
    "    \n",
    "    if total_removed > 0:\n",
    "        print(f\"\\nðŸ“‹ Removal reasons:\")\n",
    "        for reason, count in removed_reasons.items():\n",
    "            if count > 0:\n",
    "                print(f\"  â€¢ {reason.replace('_', ' ').title()}: {count}\")\n",
    "    \n",
    "    if kept_objects:\n",
    "        areas = [m['area'] for m in kept_objects]\n",
    "        compactness = [m['compactness'] for m in kept_objects]\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ Kept objects stats:\")\n",
    "        print(f\"  Area: {min(areas):,.0f} - {max(areas):,.0f} px (avg: {np.mean(areas):,.0f})\")\n",
    "        print(f\"  Compactness: {min(compactness):.3f} - {max(compactness):.3f} (avg: {np.mean(compactness):.3f})\")\n",
    "    \n",
    "    return cleaned_mask\n",
    "\n",
    "def extract_polygons_per_object(instance_mask, transform):\n",
    "    \"\"\"Extract ONE polygon per object\"\"\"\n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    polygons = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXTRACTING POLYGONS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for inst_id in unique_ids:\n",
    "        obj_mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        area = np.sum(obj_mask)\n",
    "        \n",
    "        # Extract polygon\n",
    "        for geom, val in shapes(obj_mask, mask=obj_mask, transform=transform):\n",
    "            if val > 0:\n",
    "                poly = shape(geom)\n",
    "                polygons.append({\n",
    "                    'polygon': poly,\n",
    "                    'id': int(inst_id),\n",
    "                    'area': int(area)\n",
    "                })\n",
    "                break\n",
    "    \n",
    "    print(f\"âœ“ Extracted {len(polygons)} polygons\")\n",
    "    \n",
    "    if polygons:\n",
    "        areas = [p['area'] for p in polygons]\n",
    "        print(f\"\\nPolygon Statistics:\")\n",
    "        print(f\"  Count: {len(polygons)}\")\n",
    "        print(f\"  Area range: {min(areas):,} - {max(areas):,} pixels\")\n",
    "        print(f\"  Average: {np.mean(areas):,.0f} pixels\")\n",
    "    \n",
    "    return polygons\n",
    "\n",
    "def save_geojson(polygons, output_path, crs):\n",
    "    \"\"\"Save polygons to GeoJSON\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for poly_data in polygons:\n",
    "        feature = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {\n",
    "                \"id\": poly_data['id'],\n",
    "                \"area_pixels\": poly_data['area']\n",
    "            },\n",
    "            \"geometry\": mapping(poly_data['polygon'])\n",
    "        }\n",
    "        features.append(feature)\n",
    "    \n",
    "    geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"crs\": {\n",
    "            \"type\": \"name\",\n",
    "            \"properties\": {\"name\": str(crs) if crs else \"EPSG:4326\"}\n",
    "        },\n",
    "        \"features\": features\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(geojson, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ“ GeoJSON saved: {output_path}\")\n",
    "    print(f\"  â†’ {len(features)} polygons\")\n",
    "\n",
    "def extract_boundaries(instance_mask, thickness=4):\n",
    "    \"\"\"Extract boundary for each object\"\"\"\n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    boundaries = np.zeros_like(instance_mask, dtype=np.uint8)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    \n",
    "    for inst_id in unique_ids:\n",
    "        obj_mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        edges = cv2.morphologyEx(obj_mask, cv2.MORPH_GRADIENT, kernel)\n",
    "        boundaries = np.maximum(boundaries, edges)\n",
    "    \n",
    "    if thickness > 1:\n",
    "        thick_kernel = np.ones((thickness, thickness), np.uint8)\n",
    "        boundaries = cv2.dilate(boundaries, thick_kernel, iterations=1)\n",
    "    \n",
    "    return boundaries\n",
    "\n",
    "def visualize_results(instance_mask, boundaries, polygons, save_path):\n",
    "    \"\"\"Create visualization\"\"\"\n",
    "    binary_mask = (instance_mask > 0).astype(np.uint8)\n",
    "    num_objects = len(np.unique(instance_mask)) - 1\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(18, 18))\n",
    "    \n",
    "    # 1. Instance mask (color-coded)\n",
    "    axs[0, 0].imshow(instance_mask, cmap=\"nipy_spectral\", interpolation='nearest')\n",
    "    processing_text = []\n",
    "    if USE_WATERSHED_SEPARATION:\n",
    "        processing_text.append(\"Watershed\")\n",
    "    if USE_MORPHOLOGY:\n",
    "        processing_text.append(\"Morphology\")\n",
    "    title_suffix = f\" ({' + '.join(processing_text)})\" if processing_text else \"\"\n",
    "    axs[0, 0].set_title(f\"Instance Mask{title_suffix}\\n({num_objects} high-quality objects)\", \n",
    "                        fontsize=14, fontweight='bold')\n",
    "    axs[0, 0].axis(\"off\")\n",
    "    \n",
    "    # 2. With boundaries overlay\n",
    "    rgb = np.stack([binary_mask, binary_mask, binary_mask], axis=-1).astype(float)\n",
    "    rgb[boundaries > 0] = [1.0, 0, 0]\n",
    "    \n",
    "    axs[0, 1].imshow(rgb, interpolation='nearest')\n",
    "    axs[0, 1].set_title(f\"Objects with Boundaries\\n({len(polygons)} polygons, boundaries in RED)\", \n",
    "                        fontsize=14, fontweight='bold', color='darkgreen')\n",
    "    axs[0, 1].axis(\"off\")\n",
    "    \n",
    "    # 3. Binary mask\n",
    "    axs[1, 0].imshow(binary_mask, cmap=\"gray\", interpolation='nearest')\n",
    "    axs[1, 0].set_title(f\"Binary Mask\\n(All filtered objects)\", \n",
    "                        fontsize=14, fontweight='bold')\n",
    "    axs[1, 0].axis(\"off\")\n",
    "    \n",
    "    # 4. Boundaries only\n",
    "    boundary_display = np.zeros_like(instance_mask, dtype=np.uint8)\n",
    "    boundary_display[boundaries > 0] = 255\n",
    "    \n",
    "    axs[1, 1].imshow(boundary_display, cmap=\"gray\", interpolation='nearest')\n",
    "    axs[1, 1].set_title(f\"Boundaries Only\\n({BOUNDARY_THICKNESS}px thickness)\", \n",
    "                        fontsize=14, fontweight='bold')\n",
    "    axs[1, 1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ“ Visualization saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def main(mask_path):\n",
    "    \"\"\"Main pipeline\"\"\"\n",
    "    \n",
    "    if not Path(mask_path).exists():\n",
    "        raise FileNotFoundError(f\"Mask file not found: {mask_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAM MASK â†’ FILTERED POLYGONS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nInput: {mask_path}\")\n",
    "    \n",
    "    # Load SAM mask\n",
    "    instance_mask, profile, transform, crs = load_sam_mask(mask_path)\n",
    "    \n",
    "    # Apply watershed separation to separate touching objects\n",
    "    instance_mask = apply_watershed_separation(instance_mask)\n",
    "    \n",
    "    # Apply morphological operations to improve shapes\n",
    "    instance_mask = apply_morphological_operations(instance_mask)\n",
    "    \n",
    "    # Filter objects by size and shape quality\n",
    "    instance_mask = filter_objects_by_quality(instance_mask)\n",
    "    \n",
    "    # Extract polygons (one per object)\n",
    "    polygons = extract_polygons_per_object(instance_mask, transform)\n",
    "    \n",
    "    # Extract boundaries\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXTRACTING BOUNDARIES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Thickness: {BOUNDARY_THICKNESS}px\")\n",
    "    boundaries = extract_boundaries(instance_mask, BOUNDARY_THICKNESS)\n",
    "    print(f\"âœ“ Boundary pixels: {np.sum(boundaries > 0):,}\")\n",
    "    \n",
    "    # Save files\n",
    "    profile.update({\n",
    "        'count': 1,\n",
    "        'dtype': 'uint16',\n",
    "        'compress': 'lzw',\n",
    "        'nodata': 0\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAVING FILES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    with rasterio.open(clean_mask_path, \"w\", **profile) as dst:\n",
    "        dst.write(instance_mask.astype(np.uint16), 1)\n",
    "    print(f\"âœ“ Instance mask: {clean_mask_path}\")\n",
    "    \n",
    "    if CREATE_BOUNDARY_FILE:\n",
    "        profile_boundary = profile.copy()\n",
    "        profile_boundary['dtype'] = 'uint8'\n",
    "        \n",
    "        with rasterio.open(boundaries_path, \"w\", **profile_boundary) as dst:\n",
    "            dst.write(boundaries, 1)\n",
    "        print(f\"âœ“ Boundaries: {boundaries_path}\")\n",
    "    \n",
    "    if SAVE_GEOJSON:\n",
    "        save_geojson(polygons, polygons_geojson, crs)\n",
    "    \n",
    "    # Visualize\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GENERATING VISUALIZATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    visualize_results(instance_mask, boundaries, polygons, \"polygons_visualization.png\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ“ COMPLETE!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nOutput files:\")\n",
    "    print(f\"  1. {clean_mask_path} - Filtered instance mask\")\n",
    "    print(f\"  2. {boundaries_path} - Boundaries\")\n",
    "    print(f\"  3. {polygons_geojson} - Polygons (GeoJSON) â† USE THIS!\")\n",
    "    print(f\"  4. polygons_visualization.png - Visualization\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Processing applied:\")\n",
    "    if USE_WATERSHED_SEPARATION:\n",
    "        print(f\"  â€¢ Watershed separation: separated touching objects\")\n",
    "    if USE_MORPHOLOGY:\n",
    "        print(f\"  â€¢ Morphological processing: filled holes and smoothed edges\")\n",
    "    print(f\"  â€¢ Quality filtering: removed small/irregular shapes\")\n",
    "    print(f\"\\nâœ“ Only high-quality objects with proper shapes!\")\n",
    "    \n",
    "    return instance_mask, boundaries, polygons\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         instance_mask, boundaries, polygons = main()\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nERROR: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e656ac-fade-4bf9-8ddc-28dc6eef6c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from samgeo import SamGeo2\n",
    "import cv2\n",
    "\n",
    "# Initialize SAM\n",
    "sam2 = SamGeo2(\n",
    "    model_id=\"sam2-hiera-large\",\n",
    "    apply_postprocessing=False,\n",
    "    points_per_side=32,\n",
    "    points_per_batch=64,\n",
    "    pred_iou_thresh=0.6,\n",
    "    stability_score_thresh=0.85,\n",
    "    stability_score_offset=0.7,\n",
    "    crop_n_layers=1,\n",
    "    box_nms_thresh=0.9,\n",
    "    crop_n_points_downscale_factor=2,\n",
    "    min_mask_region_area=25.0,\n",
    "    use_m2m=True,\n",
    ")\n",
    "\n",
    "def overlay_mask_downsampled(image_path, output_dir=\"outputs2\", max_size=1024, alpha=0.4, overlay_boundaries=True):\n",
    "    \"\"\"\n",
    "    Overlay instance mask on a downsampled original image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_path : str\n",
    "        Path to the original image.\n",
    "    output_dir : str\n",
    "        Directory to save intermediate outputs.\n",
    "    max_size : int\n",
    "        Maximum width or height for visualization.\n",
    "    alpha : float\n",
    "        Transparency for overlay.\n",
    "    overlay_boundaries : bool\n",
    "        Whether to overlay boundaries in red.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    mask_path = os.path.join(output_dir, f\"{base_name}_masks.tif\")\n",
    "    \n",
    "    # Generate SAM masks\n",
    "    sam2.generate(image_path)\n",
    "    sam2.save_masks(mask_path)\n",
    "    \n",
    "    # Run your main pipeline to get instance mask\n",
    "    try:\n",
    "        instance_mask, boundaries, polygons = main(mask_path)  # Assumes main() is defined elsewhere\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {mask_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    # Open original image\n",
    "    with rasterio.open(image_path) as src:\n",
    "        img = src.read([1, 2, 3])  # Assuming RGB\n",
    "        img = img.transpose(1, 2, 0).astype(float)  # H x W x C\n",
    "        img = (img - img.min()) / (img.max() - img.min())  # Normalize to 0-1\n",
    "\n",
    "        orig_h, orig_w = img.shape[:2]\n",
    "        scale = min(max_size / orig_w, max_size / orig_h, 1.0)\n",
    "\n",
    "        if scale < 1.0:\n",
    "            new_w, new_h = int(orig_w * scale), int(orig_h * scale)\n",
    "            img_ds = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "            mask_ds = cv2.resize(instance_mask.astype(np.float32), (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "            boundaries_ds = cv2.resize(boundaries.astype(np.uint8), (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "        else:\n",
    "            img_ds = img\n",
    "            mask_ds = instance_mask\n",
    "            boundaries_ds = boundaries\n",
    "\n",
    "        # Create colored overlay\n",
    "        num_objects = int(np.max(mask_ds))  # Fix TypeError by casting to int\n",
    "        if num_objects == 0:\n",
    "            overlay = img_ds\n",
    "        else:\n",
    "            import matplotlib.cm as cm\n",
    "            colors = cm.nipy_spectral(np.linspace(0, 1, num_objects + 1))[:, :3]  # RGB only\n",
    "            mask_rgb = colors[mask_ds.astype(int)]\n",
    "            overlay = (1 - alpha) * img_ds + alpha * mask_rgb\n",
    "\n",
    "        # Overlay boundaries if requested\n",
    "        if overlay_boundaries:\n",
    "            overlay[boundaries_ds > 0] = [1.0, 0, 0]  # Red boundaries\n",
    "\n",
    "        # Plot and save\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.imshow(overlay)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{base_name}: {num_objects} objects\")\n",
    "        save_path = os.path.join(output_dir, f\"{base_name}_overlay.png\")\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Overlay saved: {save_path}\")\n",
    "\n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"mask_path\": mask_path,\n",
    "        \"overlay_path\": save_path,\n",
    "        \"num_objects\": num_objects\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Main loop ----\n",
    "image_folder = \"tiff_testing\"\n",
    "results = []\n",
    "\n",
    "for file in sorted(os.listdir(image_folder)):\n",
    "    if file.lower().endswith(\".tif\"):\n",
    "        full_path = os.path.join(image_folder, file)\n",
    "        print(f\"Processing {full_path} ...\")\n",
    "        res = overlay_mask_downsampled(full_path)\n",
    "        if res:\n",
    "            results.append(res)\n",
    "\n",
    "print(\"All images processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabc26e-c08f-41ca-9990-8a852a714f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Geosam)",
   "language": "python",
   "name": "geosam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
