{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bce5e3-58f1-4601-ac59-fb9f7fa28034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36691c89-fe7f-4a85-81e0-47c3a6c2618a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import cv2\n",
    "from rasterio.features import shapes\n",
    "from shapely.geometry import shape, mapping\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Configuration ---Preprocessing/NewImagesTesting/new_test_images/tile_HK_00110.png\n",
    "IMAGE_PATH = \"new_test_images/tile_HK_00110.png\"\n",
    "mask_path = \"outputs2/tile_HK_00110_masks.tif\"\n",
    "clean_mask_path = \"masks_clean.tif\"\n",
    "boundaries_path = \"masks_boundaries.tif\"\n",
    "polygons_geojson = \"polygons.geojson\"\n",
    "\n",
    "# --- CONTAINMENT-AWARE MERGING ---\n",
    "USE_CONTAINMENT_MERGING = True\n",
    "SMALL_FRAGMENT_THRESHOLD = 10000      # Lower = merge more small fragments\n",
    "CONTAINMENT_THRESHOLD = 0.3          # Lower = more aggressive merging (30% overlap)\n",
    "TOUCH_MERGE_THRESHOLD = 0.5          # Lower = merge small fragments more easily\n",
    "\n",
    "# --- Morphological Processing ---\n",
    "USE_MORPHOLOGY = False\n",
    "MORPH_CLOSE_KERNEL = 3\n",
    "MORPH_CLOSE_ITERATIONS = 1\n",
    "\n",
    "# --- Filtering Parameters (VERY PERMISSIVE - KEEP EVERYTHING) ---\n",
    "MIN_AREA = 100                       # Very small - keep tiny regions\n",
    "MAX_AREA = 20000000\n",
    "MIN_COMPACTNESS = 0.001              # Very low - accept any shape\n",
    "MIN_SOLIDITY = 0.001                 # Very low - accept fragmented shapes\n",
    "MAX_ASPECT_RATIO = 1000.0            # Very high - accept elongated shapes\n",
    "MIN_CONVEXITY = 0.001                # Very low - accept concave shapes\n",
    "MIN_EXTENT = 0.001                   # Very low - accept sparse shapes\n",
    "\n",
    "BOUNDARY_THICKNESS = 5\n",
    "POLYGON_LINE_WIDTH = 3               # Thicker polygon lines\n",
    "CREATE_BOUNDARY_FILE = True\n",
    "SAVE_GEOJSON = True\n",
    "\n",
    "\n",
    "def load_sam_mask(mask_path):\n",
    "    \"\"\"Load SAM mask efficiently\"\"\"\n",
    "    with rasterio.open(mask_path) as src:\n",
    "        mask_data = src.read()\n",
    "        profile = src.profile.copy()\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MASK ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Shape: {mask_data.shape}, Dtype: {mask_data.dtype}\")\n",
    "    \n",
    "    if mask_data.ndim == 3 and mask_data.shape[0] > 1:\n",
    "        print(f\"âœ“ Multi-band mask: {mask_data.shape[0]} bands\")\n",
    "        instance_mask = np.zeros(mask_data.shape[1:], dtype=np.uint32)\n",
    "        \n",
    "        for band_idx in range(mask_data.shape[0]):\n",
    "            band = mask_data[band_idx]\n",
    "            if np.any(band > 0):\n",
    "                band_mask = band > 0\n",
    "                instance_mask[band_mask & (instance_mask == 0)] = band_idx + 1\n",
    "        \n",
    "        del mask_data, band, band_mask\n",
    "    else:\n",
    "        instance_mask = mask_data.squeeze().astype(np.uint32)\n",
    "        unique_vals = np.unique(instance_mask)\n",
    "        \n",
    "        if len(unique_vals[unique_vals > 0]) <= 1:\n",
    "            print(f\"âš  Binary mask - labeling connected components\")\n",
    "            instance_mask, _ = ndimage.label(instance_mask > 0)\n",
    "            instance_mask = instance_mask.astype(np.uint32)\n",
    "        \n",
    "        del mask_data\n",
    "    \n",
    "    num_instances = len(np.unique(instance_mask)) - 1\n",
    "    print(f\"âœ“ Loaded {num_instances} objects\")\n",
    "    \n",
    "    return instance_mask, profile, transform, crs\n",
    "\n",
    "\n",
    "def apply_containment_merging(instance_mask):\n",
    "    \"\"\"ULTRA AGGRESSIVE merging - merge ALL small fragments into nearest neighbor\"\"\"\n",
    "    if not USE_CONTAINMENT_MERGING:\n",
    "        return instance_mask\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ULTRA AGGRESSIVE MERGING - NO SMALL FRAGMENTS LEFT\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    # Calculate sizes\n",
    "    max_id = unique_ids.max()\n",
    "    flat_mask = instance_mask.ravel()\n",
    "    obj_sizes = np.bincount(flat_mask, minlength=max_id + 1)\n",
    "    del flat_mask\n",
    "    \n",
    "    # Classify objects\n",
    "    small_objs = unique_ids[obj_sizes[unique_ids] < SMALL_FRAGMENT_THRESHOLD]\n",
    "    large_objs = unique_ids[obj_sizes[unique_ids] >= SMALL_FRAGMENT_THRESHOLD]\n",
    "    \n",
    "    print(f\"Large objects: {len(large_objs)}, Small fragments: {len(small_objs)}\")\n",
    "    print(f\"Strategy: Merge EVERY small fragment into its nearest neighbor\")\n",
    "    \n",
    "    if len(small_objs) == 0:\n",
    "        print(\"âœ“ No small fragments to merge\")\n",
    "        return instance_mask\n",
    "    \n",
    "    # Build merge map - FORCE merge for ALL small fragments\n",
    "    merge_map = {}\n",
    "    kernel = np.ones((5, 5), np.uint8)  # LARGER kernel for better neighbor detection\n",
    "    \n",
    "    print(f\"Processing {len(small_objs)} fragments...\")\n",
    "    \n",
    "    for idx, small_id in enumerate(small_objs):\n",
    "        if idx % 50 == 0 and idx > 0:\n",
    "            print(f\"  Merged {idx}/{len(small_objs)}...\")\n",
    "        \n",
    "        small_mask = (instance_mask == small_id).astype(np.uint8)\n",
    "        coords = np.argwhere(small_mask > 0)\n",
    "        if len(coords) == 0:\n",
    "            continue\n",
    "        \n",
    "        y_min, x_min = coords.min(axis=0)\n",
    "        y_max, x_max = coords.max(axis=0)\n",
    "        \n",
    "        # Large padding to find ANY neighbor\n",
    "        pad = 10\n",
    "        y_min = max(0, y_min - pad)\n",
    "        x_min = max(0, x_min - pad)\n",
    "        y_max = min(instance_mask.shape[0], y_max + pad + 1)\n",
    "        x_max = min(instance_mask.shape[1], x_max + pad + 1)\n",
    "        \n",
    "        small_crop = small_mask[y_min:y_max, x_min:x_max]\n",
    "        mask_crop = instance_mask[y_min:y_max, x_min:x_max]\n",
    "        \n",
    "        # AGGRESSIVE dilation to find neighbors\n",
    "        dilated = cv2.dilate(small_crop, kernel, iterations=3)\n",
    "        neighbor_region = (dilated > 0) & (small_crop == 0)\n",
    "        \n",
    "        neighbor_ids = np.unique(mask_crop[neighbor_region])\n",
    "        neighbor_ids = neighbor_ids[(neighbor_ids > 0) & (neighbor_ids != small_id)]\n",
    "        \n",
    "        # If NO neighbors found with dilation, expand search further\n",
    "        if len(neighbor_ids) == 0:\n",
    "            # Try even BIGGER dilation\n",
    "            big_kernel = np.ones((11, 11), np.uint8)\n",
    "            dilated = cv2.dilate(small_crop, big_kernel, iterations=5)\n",
    "            neighbor_region = (dilated > 0) & (small_crop == 0)\n",
    "            neighbor_ids = np.unique(mask_crop[neighbor_region])\n",
    "            neighbor_ids = neighbor_ids[(neighbor_ids > 0) & (neighbor_ids != small_id)]\n",
    "        \n",
    "        if len(neighbor_ids) == 0:\n",
    "            # Still no neighbor? Merge into closest large object\n",
    "            if len(large_objs) > 0:\n",
    "                # Find closest large object by distance\n",
    "                small_coords = np.argwhere(small_mask > 0)\n",
    "                centroid = small_coords.mean(axis=0)\n",
    "                \n",
    "                min_dist = float('inf')\n",
    "                closest_large = None\n",
    "                \n",
    "                for large_id in large_objs:\n",
    "                    large_coords = np.argwhere(instance_mask == large_id)\n",
    "                    if len(large_coords) > 0:\n",
    "                        large_centroid = large_coords.mean(axis=0)\n",
    "                        dist = np.linalg.norm(centroid - large_centroid)\n",
    "                        if dist < min_dist:\n",
    "                            min_dist = dist\n",
    "                            closest_large = large_id\n",
    "                \n",
    "                if closest_large is not None:\n",
    "                    merge_map[small_id] = closest_large\n",
    "            continue\n",
    "        \n",
    "        # RULE: Merge into ANY neighbor (prefer large, but accept small too)\n",
    "        # Priority: large objects > bigger small objects\n",
    "        large_neighbors = [n for n in neighbor_ids if n in large_objs]\n",
    "        \n",
    "        if large_neighbors:\n",
    "            # Merge into largest neighbor\n",
    "            best_id = max(large_neighbors, key=lambda x: obj_sizes[x])\n",
    "            merge_map[small_id] = best_id\n",
    "        else:\n",
    "            # No large neighbors, merge into biggest small neighbor\n",
    "            best_id = max(neighbor_ids, key=lambda x: obj_sizes[x])\n",
    "            merge_map[small_id] = best_id\n",
    "        \n",
    "        del small_mask, small_crop, mask_crop, dilated, neighbor_region\n",
    "    \n",
    "    # Apply merges\n",
    "    if not merge_map:\n",
    "        print(\"âš  Warning: No merges created (this shouldn't happen)\")\n",
    "        return instance_mask\n",
    "    \n",
    "    print(f\"\\nApplying {len(merge_map)}/{len(small_objs)} merges...\")\n",
    "    \n",
    "    # Resolve transitive merges\n",
    "    def resolve_target(obj_id, visited=None):\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        if obj_id in visited or obj_id not in merge_map:\n",
    "            return obj_id\n",
    "        visited.add(obj_id)\n",
    "        return resolve_target(merge_map[obj_id], visited)\n",
    "    \n",
    "    # Apply all merges\n",
    "    for small_id, target_id in merge_map.items():\n",
    "        final_target = resolve_target(target_id)\n",
    "        instance_mask[instance_mask == small_id] = final_target\n",
    "    \n",
    "    # Renumber sequentially\n",
    "    unique_after = np.unique(instance_mask)\n",
    "    unique_after = unique_after[unique_after > 0]\n",
    "    \n",
    "    renumber_map = np.zeros(max_id + 1, dtype=np.uint32)\n",
    "    for new_id, old_id in enumerate(unique_after, start=1):\n",
    "        renumber_map[old_id] = new_id\n",
    "    \n",
    "    instance_mask = renumber_map[instance_mask]\n",
    "    \n",
    "    merged_count = len(unique_ids) - len(unique_after)\n",
    "    print(f\"âœ“ Objects: {len(unique_ids)} â†’ {len(unique_after)}\")\n",
    "    print(f\"âœ“ Merged {merged_count} small fragments ({merged_count/len(unique_ids)*100:.1f}%)\")\n",
    "    \n",
    "    # Check if any small fragments remain\n",
    "    remaining_sizes = np.bincount(instance_mask.ravel())[1:]\n",
    "    remaining_small = np.sum(remaining_sizes < SMALL_FRAGMENT_THRESHOLD)\n",
    "    if remaining_small > 0:\n",
    "        print(f\"âš  Warning: {remaining_small} small fragments still remain\")\n",
    "    else:\n",
    "        print(f\"âœ“ SUCCESS: All small fragments merged!\")\n",
    "    \n",
    "    return instance_mask\n",
    "\n",
    "\n",
    "def calculate_shape_metrics(obj_mask):\n",
    "    \"\"\"Calculate shape metrics\"\"\"\n",
    "    contours, _ = cv2.findContours(obj_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    \n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "    area = cv2.contourArea(contour)\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    \n",
    "    if area == 0 or perimeter == 0:\n",
    "        return None\n",
    "    \n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    hull = cv2.convexHull(contour)\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    hull_perimeter = cv2.arcLength(hull, True)\n",
    "    \n",
    "    return {\n",
    "        'area': area,\n",
    "        'compactness': (4 * np.pi * area) / (perimeter ** 2),\n",
    "        'solidity': area / hull_area if hull_area > 0 else 0,\n",
    "        'aspect_ratio': max(w, h) / min(w, h) if min(w, h) > 0 else 0,\n",
    "        'extent': area / (w * h) if (w * h) > 0 else 0,\n",
    "        'convexity': hull_perimeter / perimeter if perimeter > 0 else 0\n",
    "    }\n",
    "\n",
    "\n",
    "def filter_objects_by_quality(instance_mask):\n",
    "    \"\"\"Very permissive filtering - keep almost everything\"\"\"\n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FILTERING {len(unique_ids)} OBJECTS (PERMISSIVE)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    cleaned_mask = np.zeros_like(instance_mask, dtype=np.uint32)\n",
    "    new_id = 1\n",
    "    removed = {'too_small': 0, 'too_large': 0, 'poor_quality': 0}\n",
    "    \n",
    "    for inst_id in unique_ids:\n",
    "        obj_mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        metrics = calculate_shape_metrics(obj_mask)\n",
    "        \n",
    "        if metrics is None:\n",
    "            removed['too_small'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Very permissive filters\n",
    "        if metrics['area'] < MIN_AREA:\n",
    "            removed['too_small'] += 1\n",
    "        elif metrics['area'] > MAX_AREA:\n",
    "            removed['too_large'] += 1\n",
    "        elif (metrics['compactness'] < MIN_COMPACTNESS or\n",
    "              metrics['solidity'] < MIN_SOLIDITY or\n",
    "              metrics['aspect_ratio'] > MAX_ASPECT_RATIO or\n",
    "              metrics['convexity'] < MIN_CONVEXITY or\n",
    "              metrics['extent'] < MIN_EXTENT):\n",
    "            removed['poor_quality'] += 1\n",
    "        else:\n",
    "            cleaned_mask[obj_mask > 0] = new_id\n",
    "            new_id += 1\n",
    "    \n",
    "    total_kept = new_id - 1\n",
    "    total_removed = sum(removed.values())\n",
    "    \n",
    "    print(f\"âœ“ Kept: {total_kept}, Removed: {total_removed}\")\n",
    "    for reason, count in removed.items():\n",
    "        if count > 0:\n",
    "            print(f\"  â€¢ {reason}: {count}\")\n",
    "    \n",
    "    return cleaned_mask\n",
    "\n",
    "\n",
    "def extract_polygons_per_object(instance_mask, transform):\n",
    "    \"\"\"Extract polygons\"\"\"\n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXTRACTING {len(unique_ids)} POLYGONS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    polygons = []\n",
    "    \n",
    "    for inst_id in unique_ids:\n",
    "        obj_mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        area = np.count_nonzero(obj_mask)\n",
    "        \n",
    "        for geom, val in shapes(obj_mask, mask=obj_mask, transform=transform):\n",
    "            if val > 0:\n",
    "                polygons.append({\n",
    "                    'polygon': shape(geom),\n",
    "                    'id': int(inst_id),\n",
    "                    'area': int(area)\n",
    "                })\n",
    "                break\n",
    "    \n",
    "    print(f\"âœ“ Extracted {len(polygons)} polygons\")\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def save_geojson(polygons, output_path, crs):\n",
    "    \"\"\"Save polygons to GeoJSON\"\"\"\n",
    "    features = [{\n",
    "        \"type\": \"Feature\",\n",
    "        \"properties\": {\"id\": p['id'], \"area_pixels\": p['area']},\n",
    "        \"geometry\": mapping(p['polygon'])\n",
    "    } for p in polygons]\n",
    "    \n",
    "    geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"crs\": {\"type\": \"name\", \"properties\": {\"name\": str(crs) if crs else \"EPSG:4326\"}},\n",
    "        \"features\": features\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(geojson, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ“ GeoJSON saved: {output_path} ({len(features)} polygons)\")\n",
    "\n",
    "\n",
    "def extract_boundaries(instance_mask, thickness=4):\n",
    "    \"\"\"Extract boundaries\"\"\"\n",
    "    boundaries = np.zeros_like(instance_mask, dtype=np.uint8)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    \n",
    "    unique_ids = np.unique(instance_mask)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "    \n",
    "    for inst_id in unique_ids:\n",
    "        obj_mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        edges = cv2.morphologyEx(obj_mask, cv2.MORPH_GRADIENT, kernel)\n",
    "        boundaries = np.maximum(boundaries, edges)\n",
    "    \n",
    "    if thickness > 1:\n",
    "        thick_kernel = np.ones((thickness, thickness), np.uint8)\n",
    "        boundaries = cv2.dilate(boundaries, thick_kernel, iterations=1)\n",
    "    \n",
    "    return boundaries\n",
    "\n",
    "\n",
    "def load_real_image(image_path):\n",
    "    \"\"\"Load and normalize image\"\"\"\n",
    "    with rasterio.open(image_path) as src:\n",
    "        img = src.read()\n",
    "    \n",
    "    if img.shape[0] == 3:\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "    else:\n",
    "        img = img.squeeze()\n",
    "    \n",
    "    img = img.astype(np.float32)\n",
    "    img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize_results(instance_mask, boundaries, polygons, save_path, real_image=None):\n",
    "    \"\"\"Create visualization with THICK polygon lines - DIRECTLY FROM MASK\"\"\"\n",
    "    num_objects = len(np.unique(instance_mask)) - 1\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    \n",
    "    if real_image is not None:\n",
    "        # Original image\n",
    "        axs[0, 0].imshow(real_image)\n",
    "        axs[0, 0].set_title(\"Original Image\", fontsize=16, fontweight='bold')\n",
    "        axs[0, 0].axis(\"off\")\n",
    "        \n",
    "        # Image + Boundaries\n",
    "        overlay = real_image.copy()\n",
    "        if overlay.ndim == 2:\n",
    "            overlay = np.stack([overlay]*3, axis=-1)\n",
    "        overlay[boundaries > 0] = [1.0, 0.0, 0.0]\n",
    "        \n",
    "        axs[0, 1].imshow(overlay)\n",
    "        axs[0, 1].set_title(\"Image + Boundaries\", fontsize=16, fontweight='bold')\n",
    "        axs[0, 1].axis(\"off\")\n",
    "    \n",
    "    # Instance mask\n",
    "    axs[1, 0].imshow(instance_mask, cmap=\"nipy_spectral\", interpolation='nearest')\n",
    "    axs[1, 0].set_title(f\"Instance Mask ({num_objects} objects)\", fontsize=16, fontweight='bold')\n",
    "    axs[1, 0].axis(\"off\")\n",
    "    \n",
    "    # Image + POLYGONS BOUNDARIES - Draw directly from mask contours\n",
    "    if real_image is not None:\n",
    "        axs[1, 1].imshow(real_image)\n",
    "        \n",
    "        print(f\"\\nDrawing {num_objects} object boundaries on visualization...\")\n",
    "        \n",
    "        # Get unique object IDs\n",
    "        unique_ids = np.unique(instance_mask)\n",
    "        unique_ids = unique_ids[unique_ids > 0]\n",
    "        \n",
    "        # Draw contour for EACH object\n",
    "        for idx, obj_id in enumerate(unique_ids):\n",
    "            # Extract this object's mask\n",
    "            obj_mask = (instance_mask == obj_id).astype(np.uint8)\n",
    "            \n",
    "            # Find contours in PIXEL coordinates\n",
    "            contours, _ = cv2.findContours(obj_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            # Draw each contour\n",
    "            for contour in contours:\n",
    "                # Contour is in [N, 1, 2] format, reshape to [N, 2]\n",
    "                contour = contour.squeeze()\n",
    "                \n",
    "                if contour.ndim == 2 and len(contour) > 2:\n",
    "                    # Close the polygon by appending first point\n",
    "                    contour_closed = np.vstack([contour, contour[0]])\n",
    "                    \n",
    "                    # Plot with THICK lines (x, y order for matplotlib)\n",
    "                    axs[1, 1].plot(contour_closed[:, 0], contour_closed[:, 1], \n",
    "                                  color='lime', linewidth=POLYGON_LINE_WIDTH, \n",
    "                                  alpha=0.9, solid_capstyle='round', solid_joinstyle='round')\n",
    "            \n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"  Drew {idx + 1}/{num_objects} objects...\")\n",
    "        \n",
    "        axs[1, 1].set_title(f\"Image + {num_objects} Object Boundaries (Thick Lines)\", \n",
    "                           fontsize=16, fontweight='bold')\n",
    "        axs[1, 1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "    print(f\"âœ“ Visualization saved: {save_path}\")\n",
    "    \n",
    "    # SHOW in notebook\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def main(mask_path,image_path):\n",
    "    \"\"\"Main pipeline\"\"\"\n",
    "    if not Path(mask_path).exists():\n",
    "        raise FileNotFoundError(f\"Mask file not found: {mask_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAM MASK â†’ FILTERED POLYGONS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load\n",
    "    instance_mask, profile, transform, crs = load_sam_mask(mask_path)\n",
    "    \n",
    "    # Merge (aggressive)\n",
    "    instance_mask = apply_containment_merging(instance_mask)\n",
    "    \n",
    "    # Filter (very permissive)\n",
    "    instance_mask = filter_objects_by_quality(instance_mask)\n",
    "    \n",
    "    # Extract\n",
    "    polygons = extract_polygons_per_object(instance_mask, transform)\n",
    "    boundaries = extract_boundaries(instance_mask, BOUNDARY_THICKNESS)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL POLYGON COUNT: {len(polygons)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Save\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SAVING FILES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    profile.update({'count': 1, 'dtype': 'uint32', 'compress': 'lzw', 'nodata': 0})\n",
    "    \n",
    "    with rasterio.open(clean_mask_path, \"w\", **profile) as dst:\n",
    "        dst.write(instance_mask, 1)\n",
    "    print(f\"âœ“ Instance mask: {clean_mask_path}\")\n",
    "    \n",
    "    if CREATE_BOUNDARY_FILE:\n",
    "        profile['dtype'] = 'uint8'\n",
    "        with rasterio.open(boundaries_path, \"w\", **profile) as dst:\n",
    "            dst.write(boundaries, 1)\n",
    "        print(f\"âœ“ Boundaries: {boundaries_path}\")\n",
    "    \n",
    "    if SAVE_GEOJSON:\n",
    "        save_geojson(polygons, polygons_geojson, crs)\n",
    "    \n",
    "    # Visualize\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"VISUALIZATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    real_image = load_real_image(image_path)\n",
    "    fig = visualize_results(instance_mask, boundaries, polygons, \"polygons_visualization.png\", real_image)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ“ COMPLETE!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nðŸ“Š Summary:\")\n",
    "    print(f\"  â€¢ Total polygons: {len(polygons)}\")\n",
    "    print(f\"  â€¢ GeoJSON features: {len(polygons)}\")\n",
    "    print(f\"  â€¢ All polygons drawn with {POLYGON_LINE_WIDTH}px thick lines\")\n",
    "    print(f\"  â€¢ NO small regions removed - everything merged\")\n",
    "    \n",
    "    return instance_mask, boundaries, polygons, real_image, fig\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        instance_mask, boundaries, polygons, real_image, fig = main()\n",
    "        # instance_mask, boundaries, polygons, real_image, fig = main()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89439561-f7fb-4fdc-a7dd-b049668dcd79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from samgeo import SamGeo2\n",
    "import cv2\n",
    "\n",
    "# Initialize SAM\n",
    "sam2 = SamGeo2(\n",
    "    model_id=\"sam2-hiera-large\",\n",
    "    apply_postprocessing=True,\n",
    "    points_per_side=64,        \n",
    "    points_per_batch=128,\n",
    "    pred_iou_thresh=0.7,         \n",
    "    stability_score_thresh=0.85,\n",
    "    stability_score_offset=0.7,\n",
    "    crop_n_layers=1,          \n",
    "    box_nms_thresh=0.8,\n",
    "    min_mask_region_area=10.0,   \n",
    "    use_m2m=True,              \n",
    ")\n",
    "\n",
    "\n",
    "def overlay_mask_downsampled(image_path, output_dir=\"outputs2\", max_size=1024, alpha=0.4, overlay_boundaries=True):\n",
    "    \"\"\"\n",
    "    Overlay instance mask on a downsampled original image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_path : str\n",
    "        Path to the original image.\n",
    "    output_dir : str\n",
    "        Directory to save intermediate outputs.\n",
    "    max_size : int\n",
    "        Maximum width or height for visualization.\n",
    "    alpha : float\n",
    "        Transparency for overlay.\n",
    "    overlay_boundaries : bool\n",
    "        Whether to overlay boundaries in red.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    mask_path = os.path.join(output_dir, f\"{base_name}_masks.tif\")\n",
    "    \n",
    "    # Generate SAM masks\n",
    "    sam2.generate(image_path)\n",
    "    sam2.save_masks(mask_path)\n",
    "    sam2.show_anns(\n",
    "    axis=\"off\",\n",
    "    alpha=0.7)\n",
    "    # Run your main pipeline to get instance mask\n",
    "    try:\n",
    "        instance_mask, boundaries, polygons, real_image, fig = main(mask_path,image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {mask_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "    # Open original image\n",
    "    with rasterio.open(image_path) as src:\n",
    "        img = src.read([1, 2, 3])  # Assuming RGB\n",
    "        img = img.transpose(1, 2, 0).astype(float)  # H x W x C\n",
    "        img = (img - img.min()) / (img.max() - img.min())  # Normalize to 0-1\n",
    "\n",
    "        orig_h, orig_w = img.shape[:2]\n",
    "        scale = min(max_size / orig_w, max_size / orig_h, 1.0)\n",
    "\n",
    "        if scale < 1.0:\n",
    "            new_w, new_h = int(orig_w * scale), int(orig_h * scale)\n",
    "            img_ds = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "            mask_ds = cv2.resize(instance_mask.astype(np.float32), (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "            boundaries_ds = cv2.resize(boundaries.astype(np.uint8), (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "        else:\n",
    "            img_ds = img\n",
    "            mask_ds = instance_mask\n",
    "            boundaries_ds = boundaries\n",
    "\n",
    "        # Create colored overlay\n",
    "        num_objects = int(np.max(mask_ds))  # Fix TypeError by casting to int\n",
    "        if num_objects == 0:\n",
    "            overlay = img_ds\n",
    "        else:\n",
    "            import matplotlib.cm as cm\n",
    "            colors = cm.nipy_spectral(np.linspace(0, 1, num_objects + 1))[:, :3]  # RGB only\n",
    "            mask_rgb = colors[mask_ds.astype(int)]\n",
    "            overlay = (1 - alpha) * img_ds + alpha * mask_rgb\n",
    "\n",
    "        # Overlay boundaries if requested\n",
    "        if overlay_boundaries:\n",
    "            overlay[boundaries_ds > 0] = [1.0, 0, 0]  # Red boundaries\n",
    "\n",
    "        # Plot and save\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.imshow(overlay)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{base_name}: {num_objects} objects\")\n",
    "        save_path = os.path.join(output_dir, f\"{base_name}_overlay.png\")\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Overlay saved: {save_path}\")\n",
    "\n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"mask_path\": mask_path,\n",
    "        \"overlay_path\": save_path,\n",
    "        \"num_objects\": num_objects\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Main loop ----\n",
    "image_folder = \"new_test_images\"\n",
    "results = []\n",
    "\n",
    "for file in sorted(os.listdir(image_folder)):\n",
    "    if file.lower().endswith(\".png\"):\n",
    "        full_path = os.path.join(image_folder, file)\n",
    "        print(f\"Processing {full_path} ...\")\n",
    "        res = overlay_mask_downsampled(full_path)\n",
    "        if res:\n",
    "            results.append(res)\n",
    "\n",
    "print(\"All images processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b4d64-fffb-4c03-b822-11ff49ca022a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36dc27f-e8c3-4a05-9a4d-95ae9a36ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 2155382"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dd103-b4d7-445a-b144-29ce8b5f1e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kill -9 2461153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066521c-8072-42a6-b455-b9ee9fc11f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Geosam)",
   "language": "python",
   "name": "geosam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
