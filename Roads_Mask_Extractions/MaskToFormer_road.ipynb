{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f3c4e4a-fdfb-4ed7-8019-f001b16a16ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from transformers import Mask2FormerImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd32301-f81f-4e7a-ad3d-76b1d09c0c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"MODEL_CHECKPOINT\": \"facebook/mask2former-swin-tiny-coco-instance\",\n",
    "    \"WEIGHTS_PATH\": \"mask2former.pth\",\n",
    "    \"INFER_SIZE\": 512,     # model input\n",
    "    \"ORIG_SIZE\": 2048,    # visualization size\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "ROAD_CLASS_ID = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c0230-5b6f-49be-8450-b5312c341801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor = Mask2FormerImageProcessor.from_pretrained(\n",
    "    CONFIG[\"MODEL_CHECKPOINT\"],\n",
    "    do_resize=False,\n",
    "    do_rescale=True,\n",
    "    do_normalize=True\n",
    ")\n",
    "\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "    CONFIG[\"MODEL_CHECKPOINT\"],\n",
    "    num_labels=7,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "state_dict = torch.load(CONFIG[\"WEIGHTS_PATH\"], map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(CONFIG[\"DEVICE\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4ee0c30-d4d6-404d-8999-91876ff4fef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resize_to_model = A.Compose([\n",
    "    A.Resize(CONFIG[\"INFER_SIZE\"], CONFIG[\"INFER_SIZE\"])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc86b35-9e97-4856-bbed-105f882aaac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22a18cf0-2d86-4ed0-a306-5d127f635c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_road_mask_2048(image_path):\n",
    "    # Load original image (2048×2048)\n",
    "    orig_image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "    h, w = orig_image.shape[:2]\n",
    "\n",
    "    assert h == CONFIG[\"ORIG_SIZE\"] and w == CONFIG[\"ORIG_SIZE\"], \\\n",
    "        \"Image must be 2048×2048\"\n",
    "\n",
    "    # Resize for model\n",
    "    resized = resize_to_model(image=orig_image)[\"image\"]\n",
    "\n",
    "    # HF processor\n",
    "    inputs = processor(\n",
    "        images=resized,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(CONFIG[\"DEVICE\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Semantic segmentation\n",
    "    semantic_map = processor.post_process_semantic_segmentation(\n",
    "        outputs,\n",
    "        target_sizes=[(CONFIG[\"INFER_SIZE\"], CONFIG[\"INFER_SIZE\"])]\n",
    "    )[0].cpu().numpy()\n",
    "\n",
    "    # Road vs Others\n",
    "    road_mask_small = (semantic_map == ROAD_CLASS_ID).astype(np.uint8)\n",
    "\n",
    "    # Upscale to 2048×2048\n",
    "    road_mask = cv2.resize(\n",
    "        road_mask_small,\n",
    "        (CONFIG[\"ORIG_SIZE\"], CONFIG[\"ORIG_SIZE\"]),\n",
    "        interpolation=cv2.INTER_NEAREST\n",
    "    )\n",
    "\n",
    "    return orig_image, road_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9a6f8dc-ef0d-483a-8359-4820ee96ebfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_matplotlib(orig_image, road_mask):\n",
    "    \"\"\"\n",
    "    orig_image : RGB image (H, W, 3)\n",
    "    road_mask  : binary mask (H, W), 1 = road\n",
    "    \"\"\"\n",
    "\n",
    "    # Create overlay (RGB)\n",
    "    overlay = orig_image.copy()\n",
    "    overlay[road_mask == 1] = [255, 0, 0]  # RED roads in RGB\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    #Original Image\n",
    "    axes[0].imshow(orig_image)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    #Road Mask\n",
    "    axes[1].imshow(road_mask, cmap=\"gray\")\n",
    "    axes[1].set_title(\"Road Mask\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    #Overlay\n",
    "    axes[2].imshow(overlay)\n",
    "    axes[2].set_title(\"Road Overlay\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54296ce3-a56e-47d2-831f-62c64809bb68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = \"roads/tile_10240_47104.tif\"\n",
    "\n",
    "orig_image, road_mask = predict_road_mask_2048(image_path)\n",
    "visualize_matplotlib(orig_image, road_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01f3d6c0-f81b-4f91-822e-ff32f1bae51b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "def run_folder_inference(\n",
    "    folder_path,\n",
    "    extensions=(\".tif\", \".tiff\"),\n",
    "    pause=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Loops through folder, runs road segmentation, and visualizes results.\n",
    "\n",
    "    pause=True  → waits for key press between images\n",
    "    pause=False → auto-advance\n",
    "    \"\"\"\n",
    "\n",
    "    image_files = sorted([\n",
    "        os.path.join(folder_path, f)\n",
    "        for f in os.listdir(folder_path)\n",
    "        if f.lower().endswith(extensions)\n",
    "    ])\n",
    "\n",
    "    print(f\"Found {len(image_files)} images\")\n",
    "\n",
    "    for idx, image_path in enumerate(image_files, 1):\n",
    "        print(f\"\\n[{idx}/{len(image_files)}] Processing: {os.path.basename(image_path)}\")\n",
    "\n",
    "        try:\n",
    "            orig_image, road_mask = predict_road_mask_2048(image_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed on {image_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        visualize_matplotlib(orig_image, road_mask)\n",
    "\n",
    "        if pause:\n",
    "            input(\"Press ENTER to continue (Ctrl+C to stop)...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b6fb9-80a3-42e4-bc6b-c9a835334cec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_path = \"roads/\"\n",
    "\n",
    "run_folder_inference(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5613f4-9cc4-4464-a2b9-263276aab43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (preprocess)",
   "language": "python",
   "name": "preprocesss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
